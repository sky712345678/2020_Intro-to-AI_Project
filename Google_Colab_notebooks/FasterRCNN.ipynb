{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FasterRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03324b3f632f4472981fba81f1b62ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6341ea61ce714dbf8954e982a31b709a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e356708c4b3c4f1e9f9a9a3a3a8e726a",
              "IPY_MODEL_1594c88f58fd4b29b8bd3dd0171f6372"
            ]
          }
        },
        "6341ea61ce714dbf8954e982a31b709a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e356708c4b3c4f1e9f9a9a3a3a8e726a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d52b5175baa3447180baf6b4883f33e8",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178090079,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178090079,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00a786c2130f443bb47956cd292a6cdc"
          }
        },
        "1594c88f58fd4b29b8bd3dd0171f6372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ec80547085e447da70c870f0a069256",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:00&lt;00:00, 211MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1964cc918fd444a9cbedbed55e1028f"
          }
        },
        "d52b5175baa3447180baf6b4883f33e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00a786c2130f443bb47956cd292a6cdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ec80547085e447da70c870f0a069256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1964cc918fd444a9cbedbed55e1028f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrjycWTr3g7J",
        "outputId": "438bbdb7-9840-45f8-8dcc-a9f13a5873bb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec 15 15:29:11 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OnQhKL-3DgO",
        "outputId": "27ba5817-5cf1-4406-995b-1bceb62a2b8f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFRHsCuMqeTC"
      },
      "source": [
        "---\r\n",
        "Modified Fasterrcnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv8Aa_UIyfPN"
      },
      "source": [
        "!mkdir drive/MyDrive/AIIntro/[processed]dream_picture/20200610-20200620-camera-3/tuning_photos/Sharpen_0p4_Add_5/Maskrcnn_test_thresh-0p35/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "DYdlNFrWybVr",
        "outputId": "1f9c8dfd-f563-4974-e70e-a7dcc4c4e963"
      },
      "source": [
        "import gdown\r\n",
        "url = 'https://drive.google.com/uc?export=download&id=170CjxBy-5nkoCmVzkoSfOXcGxZ-ZqwET'\r\n",
        "output = 'drive/MyDrive/AIIntro/[processed]dream_picture/20200610-20200620-camera-3/tuning_photos/Sharpen_0p4_Add_5/Maskrcnn_test_thresh-0p35/result.json'\r\n",
        "gdown.download(url, output, quiet=False) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=170CjxBy-5nkoCmVzkoSfOXcGxZ-ZqwET\n",
            "To: /content/drive/MyDrive/AIIntro/[processed]dream_picture/20200610-20200620-camera-3/tuning_photos/Sharpen_0p4_Add_5/Maskrcnn_test_thresh-0p35/result.json\n",
            "0.00B [00:00, ?B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/AIIntro/[processed]dream_picture/20200610-20200620-camera-3/tuning_photos/Sharpen_0p4_Add_5/Maskrcnn_test_thresh-0p35/result.json'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgPUde99P5E9"
      },
      "source": [
        "# MODIFY THE THRESHOLD OF FASTERRCNN TO 0.4\r\n",
        "# DO DETECTION WITH Sharp_0p4_Add_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "03324b3f632f4472981fba81f1b62ef7",
            "6341ea61ce714dbf8954e982a31b709a",
            "e356708c4b3c4f1e9f9a9a3a3a8e726a",
            "1594c88f58fd4b29b8bd3dd0171f6372",
            "d52b5175baa3447180baf6b4883f33e8",
            "00a786c2130f443bb47956cd292a6cdc",
            "3ec80547085e447da70c870f0a069256",
            "f1964cc918fd444a9cbedbed55e1028f"
          ]
        },
        "id": "NApDgShbJC0a",
        "outputId": "55545ec1-1209-4729-e3bf-0a36e3695a0f"
      },
      "source": [
        "# Copyright (c) 2017-present, Facebook, Inc.\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "\r\n",
        "# Function `vis_bbox`, `vis_mask`, and `vis_class` are adapted from: \r\n",
        "# https://github.com/facebookresearch/Detectron/blob/7aa91aaa5a85598399dc8d8413e05a06ca366ba7/detectron/utils/vis.py\r\n",
        "\r\n",
        "\"\"\"PyTorch object detection example.\"\"\"\r\n",
        "\r\n",
        "import imageio\r\n",
        "import os\r\n",
        "import json\r\n",
        "\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torchvision.models as models\r\n",
        "import torchvision.transforms as trns\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "_GRAY = (218, 227, 218)\r\n",
        "_GREEN = (18, 127, 15)\r\n",
        "_WHITE = (255, 255, 255)\r\n",
        "\r\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\r\n",
        "    \"__background__\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\",\r\n",
        "    \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"N/A\", \"stop sign\",\r\n",
        "    \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\r\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"N/A\", \"backpack\", \"umbrella\", \"N/A\", \"N/A\",\r\n",
        "    \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\r\n",
        "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\r\n",
        "    \"bottle\", \"N/A\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\",\r\n",
        "    \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\",\r\n",
        "    \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"N/A\", \"dining table\",\r\n",
        "    \"N/A\", \"N/A\", \"toilet\", \"N/A\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\r\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"N/A\", \"book\",\r\n",
        "    \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "def vis_bbox(image, bbox, color=_GREEN, thick=1):\r\n",
        "    \"\"\"Visualizes a bounding box.\"\"\"\r\n",
        "    image = image.astype(np.uint8)\r\n",
        "    bbox = list(map(int, bbox))\r\n",
        "    x0, y0, x1, y1 = bbox\r\n",
        "    cv2.rectangle(image, (x0, y0), (x1, y1), color, thickness=thick)\r\n",
        "    return image\r\n",
        "\r\n",
        "\r\n",
        "def vis_mask(image, mask, col, alpha=0.4, show_border=True, border_thick=1):\r\n",
        "    \"\"\"Visualizes a single binary mask.\"\"\"\r\n",
        "    image = image.astype(np.float32)\r\n",
        "\r\n",
        "    mask = mask >= 0.5\r\n",
        "    mask = mask.astype(np.uint8)\r\n",
        "    idx = np.nonzero(mask)\r\n",
        "\r\n",
        "    image[idx[0], idx[1], :] *= 1.0 - alpha\r\n",
        "    image[idx[0], idx[1], :] += alpha * col\r\n",
        "\r\n",
        "    if show_border:\r\n",
        "        contours = cv2.findContours(\r\n",
        "            mask.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)[-2]\r\n",
        "        cv2.drawContours(image, contours, -1, _WHITE,\r\n",
        "                         border_thick, cv2.LINE_AA)\r\n",
        "\r\n",
        "    return image.astype(np.uint8)\r\n",
        "\r\n",
        "\r\n",
        "def vis_class(image, bbox, text, bg_color=_GREEN, text_color=_GRAY, font_scale=0.35):\r\n",
        "    \"\"\"Visualizes the class.\"\"\"\r\n",
        "    image = image.astype(np.uint8)\r\n",
        "    x0, y0 = int(bbox[0]), int(bbox[1])\r\n",
        "\r\n",
        "    # Compute text size\r\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\r\n",
        "    ((text_w, text_h), _) = cv2.getTextSize(text, font, font_scale, 1)\r\n",
        "\r\n",
        "    # Place text background\r\n",
        "    back_tl = x0, y0 - int(1.3 * text_h)\r\n",
        "    back_br = x0 + text_w, y0\r\n",
        "    cv2.rectangle(image, back_tl, back_br, bg_color, -1)\r\n",
        "\r\n",
        "    # Show text\r\n",
        "    text_tl = x0, y0 - int(0.3 * text_h)\r\n",
        "    cv2.putText(image, text, text_tl, font, font_scale,\r\n",
        "                text_color, lineType=cv2.LINE_AA)\r\n",
        "\r\n",
        "    return image\r\n",
        "\r\n",
        "\r\n",
        "def run_object_detection(model, image_path, transforms, output_path, threshold):\r\n",
        "    \"\"\"Inference.\"\"\"\r\n",
        "    ### modified by sky712345678, add support for run object detection in batch\r\n",
        "    ### modified by sky712345678, add support for generate detection results in JSON format\r\n",
        "    ### modified by sky712345678, save detection result with imageio to make this easier to use in Google Colab\r\n",
        "    # os.makedirs(os.path.dirname(output_path), exist_ok=True)\r\n",
        "    \r\n",
        "    imageAddress = []\r\n",
        "    for f in os.listdir(image_path):\r\n",
        "        if(f.__contains__('.jpg')):\r\n",
        "            imageAddress.append(image_path + '/' + f)\r\n",
        "    \r\n",
        "    imageAddress.sort()\r\n",
        "\r\n",
        "    json_result = []\r\n",
        "    frame_id = 1\r\n",
        "\r\n",
        "    for f in imageAddress:\r\n",
        "        # Read image and run prepro\r\n",
        "        image = Image.open(f).convert(\"RGB\")\r\n",
        "        image_tensor = transforms(image)\r\n",
        "        # print(f\"\\n\\nImage size after transformation: {image_tensor.size()}\")\r\n",
        "\r\n",
        "        image_tensor = image_tensor.to(device)\r\n",
        "\r\n",
        "        # Feed input and get results at index 0\r\n",
        "        # (input image is at index 0 in the list)\r\n",
        "        outputs = model([image_tensor])[0]\r\n",
        "\r\n",
        "        # Result postpro and vis\r\n",
        "        display_image = np.array(image)\r\n",
        "        outputs = {k: v.cpu().numpy() for k, v in outputs.items()}\r\n",
        "        is_mask = True if \"masks\" in outputs else False\r\n",
        "        if is_mask:\r\n",
        "            outputs[\"masks\"] = np.squeeze(outputs[\"masks\"], axis=1)\r\n",
        "\r\n",
        "        # print(\"\\n\\nInference results:\")\r\n",
        "        json_objects = []\r\n",
        "        for i, (bbox, label, score) in enumerate(zip(outputs[\"boxes\"], outputs[\"labels\"], outputs[\"scores\"])):\r\n",
        "            if score < threshold:\r\n",
        "                continue\r\n",
        "\r\n",
        "            # print(f\"Label {label}: {COCO_INSTANCE_CATEGORY_NAMES[label]} ({score:.2f})\")\r\n",
        "\r\n",
        "            object_dict = {\r\n",
        "                \"class_id\": int(label),\r\n",
        "                \"name\": COCO_INSTANCE_CATEGORY_NAMES[label],\r\n",
        "                \"confidence\": float(score)\r\n",
        "            }\r\n",
        "            json_objects.append(object_dict)\r\n",
        "\r\n",
        "            display_image = vis_bbox(display_image, bbox)\r\n",
        "            display_image = vis_class(\r\n",
        "                display_image, bbox, COCO_INSTANCE_CATEGORY_NAMES[label])\r\n",
        "            if is_mask:\r\n",
        "                display_image = vis_mask(\r\n",
        "                    display_image, outputs[\"masks\"][i], np.array([0., 0., 255.]))\r\n",
        "\r\n",
        "        '''plt.figure(figsize=(10, 6))\r\n",
        "        plt.imshow(display_image)\r\n",
        "        plt.xticks([])\r\n",
        "        plt.yticks([])\r\n",
        "        plt.savefig(output_path + '/' + f, bbox_inches=\"tight\")'''\r\n",
        "        imageio.imsave(f.replace(image_path, output_path), display_image)\r\n",
        "\r\n",
        "        result_dict = {\r\n",
        "            \"frame_id\": frame_id,\r\n",
        "            \"filename\": f,\r\n",
        "            \"objects\": json_objects\r\n",
        "        }\r\n",
        "        json_result.append(result_dict)\r\n",
        "        frame_id += 1\r\n",
        "\r\n",
        "    json_resultDump = json.dumps(json_result, indent=4)\r\n",
        "    with open(output_path + '/result.json', 'w') as outfile:\r\n",
        "        outfile.write(json_resultDump)\r\n",
        "    print('\\n\\ncheck result file saved successfully!')\r\n",
        "    ### modified by sky712345678, add support for run object detection in batch\r\n",
        "    ### modified by sky712345678, add support for generate detection results in JSON format\r\n",
        "    ### modified by sky712345678, save detection result with imageio to make this easier to use in Google Colab\r\n",
        "\r\n",
        "    return\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    ### modified by sky712345678, comment out argparse because it isn't useful in Google Colab\r\n",
        "    # parser = argparse.ArgumentParser(\"PyTorch Object Detection\")\r\n",
        "    # parser.add_argument(\"--image_path\", type=str,\r\n",
        "    #                     default=\"images/sheep-herd-shepherd-hats-dog-meadow.jpg\", help=\"path to image\")\r\n",
        "    # parser.add_argument(\"--model_type\", type=str,\r\n",
        "    #                     default=\"fasterrcnn\", help=\"fasterrcnn or maskrcnn\")\r\n",
        "    # parser.add_argument(\"--output_path\", type=str,\r\n",
        "    #                     default=\"out.png\", help=\"path to save output image\")\r\n",
        "\r\n",
        "    # Parse arguments\r\n",
        "    # args = parser.parse_args()\r\n",
        "\r\n",
        "    image_path = 'drive/MyDrive/AIIntro/[processed]dream_picture/20200610-20200620-camera-3/tuning_photos/Sharpen_0p4_Add_5'\r\n",
        "    model_type = 'maskrcnn'\r\n",
        "    output_path = 'drive/MyDrive/AIIntro/[processed]dream_picture/20200610-20200620-camera-3/tuning_photos/Sharpen_0p4_Add_5/Maskrcnn_test_thresh-0p35'\r\n",
        "    threshold = 0.35\r\n",
        "    ### modified by sky712345678, comment out argparse because it isn't useful in Google Colab\r\n",
        "\r\n",
        "    # Define image transforms\r\n",
        "    transforms = trns.ToTensor()\r\n",
        "\r\n",
        "    # Load model\r\n",
        "    if model_type == \"fasterrcnn\":\r\n",
        "        net = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\r\n",
        "    elif model_type == \"maskrcnn\":\r\n",
        "        net = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\r\n",
        "    else:\r\n",
        "        raise AssertionError\r\n",
        "\r\n",
        "    ### modified by sky12345678, add GPU acceleration support\r\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    model = net.to(device)\r\n",
        "    ### modified by sky12345678, add GPU acceleration support\r\n",
        "\r\n",
        "    # print(model)\r\n",
        "\r\n",
        "    # Set model to eval mode\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "\r\n",
        "    # Run model\r\n",
        "    with torch.no_grad():\r\n",
        "        run_object_detection(model, image_path, transforms, output_path, threshold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03324b3f632f4472981fba81f1b62ef7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "check result file saved successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ball-7Ey6UM"
      },
      "source": [
        "---\r\n",
        "**jwyang faster rcnn (unusable)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc_qIkPk238K"
      },
      "source": [
        "!git clone -b pytorch-1.0 https://github.com/jwyang/faster-rcnn.pytorch.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_8ukEbA3Qok"
      },
      "source": [
        "cd faster-rcnn.pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKPnGrRWy8Z5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqwi-HZa3Rq5"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in1dxMp-ATQb"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwRQC6h03mFX"
      },
      "source": [
        "!wget https://www.dropbox.com/s/iev3tkbz5wyyuz9/resnet101_caffe.pth?dl=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyE5moSw7OND"
      },
      "source": [
        "!wget https://www.dropbox.com/s/s3brpk0bdq60nyb/vgg16_caffe.pth?dl=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0Q7fuDABAFS"
      },
      "source": [
        "!mv vgg16_caffe.pth?dl=0 vgg16.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DUoIlfOBlNb"
      },
      "source": [
        "!mkdir data/pretrained_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6aLCFqLBwlU"
      },
      "source": [
        "!mv vgg16.pth data/pretrained_model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8OCpQDw4Xdn"
      },
      "source": [
        "cd lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d8eUDVD4YuO"
      },
      "source": [
        "!python setup.py build develop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5-I7RNx7MCn"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wNXec9k8qo7"
      },
      "source": [
        "!pip install scipy==1.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvuIMqeJ7Vrk"
      },
      "source": [
        "!python demo.py --net vgg16 \\\r\n",
        "               --checksession 1 --checkepoch 6 --checkpoint 416 \\\r\n",
        "               --cuda --load_dir data/pretrained_mode"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}